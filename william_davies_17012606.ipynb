{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NQVCToZkRV8"
   },
   "source": [
    "# COMP0124 MAAI Individual Coursework \n",
    "\n",
    "This 50-point individual coursework has four parts,\n",
    "the Matrix Game, the Stochastic Game, the Nonzero-sum Game and Deep Multi-Agent Reinforcement Learning.\n",
    "\n",
    "## Instructions\n",
    "1.   To start this CW, please duplicate this notebook at first:\n",
    "  - Choose \"File => Save a copy in Drive\" and open/run it in Colab.\n",
    "  - Or you can download the notebook and run it in your local jupyter notebook server.\n",
    "2.   For the coding assignment, please write your code at `### TODO ###` blocks or in a new cell. For analysis report, you are free to use as many blocks as you need.\n",
    "3.   Before submitting your notebook, **make sure that it runs without errors**, we also provide a validation tool in the end of this notebook.\n",
    "  - To check this, reload your notebook and the Python kernel, and run the notebook from the first to the last cell.\n",
    "  - Please do not change any methods or variables' name in the notebook, otherwise, you cannot get marking correctly.\n",
    "  - We would not help you debug the code, if we cannot run your submitted notebook, you will get zero point. \n",
    "4.  Download your notebook and submit it on Moodle.\n",
    "  - Click on \"File -> Download .ipynb\".\n",
    "  - Rename your notebook to ***firstname_lastname_studentnumber.ipynb***. (Please strictly follow the naming requirement.)\n",
    "  - Upload to Moodle.\n",
    "5. This CW would due by **23:55 26/03/2021**, please submit your .ipynb file through the [submission entrance](https://moodle.ucl.ac.uk/mod/assign/view.php?id=1685901).\n",
    "6. If you have any questions, please contact TAs: [Minne Li](minne.li@cs.ucl.ac.uk), [Oliver Slumbers](o.slumbers@cs.ucl.ac.uk), [Xihan Li](xihan.li.20@ucl.ac.uk), [Xidong Feng](xidong.feng@cs.ucl.ac.uk), and [Mengyue Yang](m.yang@cs.ucl.ac.uk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvk15WL4lisG"
   },
   "source": [
    "## Part I: Matrix Game (10 points)\n",
    "\n",
    "We start with the simplest setting: Matrix Game (a.k.a Stage Game/Normal Form Game). In this part, you will try to solve the matrix game with full knowledge of the payoff for each player in the game.\n",
    "\n",
    "\n",
    "\n",
    "Given a two-player, two-action matrix game, we have the payoff matrices as follows:\n",
    "$$\n",
    "\\mathbf{R}^1 = \\left[\\begin{matrix}\n",
    "0 & 3 \\\\\n",
    "1 &2\n",
    "\\end{matrix}\\right] \n",
    "\\quad \n",
    "\\mathbf{R}^2 = \\left[\\begin{matrix}\n",
    "3 & 2 \\\\\n",
    "0 & 1\n",
    "\\end{matrix}\\right]\n",
    "$$\n",
    "\n",
    "Each player selects an action from the action space $\\{1,2\\}$ which determines the payoffs to the players. If the player 1 chooses action $i$ and the player 2 chooses action $j$, then the player 1 and player2 receive the rewards $r^1_{ij}$ and $r^2_{ij}$ respectively. For example, if both players choose action $1$, then the player 1 would have $r^1_{11}=0$ and player 1 would receive $r^2_{11}=3$.\n",
    "\n",
    "Then, we can use $\\alpha\\in [0,1] $ represents the strategy for player 1, where $\\alpha$ corresponds to the probability of player 1 selecting the first action (action 1), and $1-\\alpha$ is the probability of choosing the second action (action 2). Similarly, we use $\\beta$ to be the strategy for player 2.\n",
    "\n",
    "Given the pair of strategies $(\\alpha, \\beta)$, we can have the expected payoffs for two players. Denote $V^1(\\alpha, \\beta)$ and $V^2(\\alpha, \\beta)$ as the expected payoffs for two players respectively:\n",
    "\n",
    "$$\n",
    "\\begin{aligned} V^{1}(\\alpha, \\beta) &=\\alpha \\beta r^1_{11}+\\alpha(1-\\beta) r^1_{12}+(1-\\alpha) \\beta r^1_{21}+(1-\\alpha)(1-\\beta) r^1_{22} \\\\ &=u^1 \\alpha \\beta+\\alpha\\left(r^1_{12}-r^1_{22}\\right)+\\beta\\left(r^1_{21}-r^1_{22}\\right)+r^1_{22} \\end{aligned}\n",
    "$$\n",
    "$$\n",
    "\\begin{aligned} V^{2}(\\alpha, \\beta) &=\\alpha \\beta r^2_{11}+\\alpha(1-\\beta) r^2_{12}+(1-\\alpha) \\beta r^2_{21}+(1-\\alpha)(1-\\beta) r^2_{22} \\\\ &=u^2 \\alpha \\beta+\\alpha\\left(r^2_{12}-r^2_{22}\\right)+\\beta\\left(r^1_{21}-r^2_{22}\\right)+r^2_{22}\\end{aligned}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{aligned} u^1 &=r^1_{11}-r^1_{12}-r^1_{21}+r^1_{22} \\\\  u^2 &=r^2_{11}-r^2_{12}-r^2_{21}+r^2_{22} .\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Buu2AbE3oQR3"
   },
   "source": [
    "#### Set up matrix game (4 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1PijoUYXdHG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def U(payoff):\n",
    "    ########### TODO: Compute u (1 point) ###########\n",
    "    u = _\n",
    "    ########### END TODO ############################\n",
    "    return u\n",
    "  \n",
    "    \n",
    "# expected payoff\n",
    "def V(alpha, beta, payoff):\n",
    "    ########### TODO: Compute expected payoff of given strategies alpha and beta (1 point) ###########\n",
    "    u = U(payoff)\n",
    "    v = _\n",
    "    ########### END TODO ##############################################################################\n",
    "    return v\n",
    "\n",
    "\n",
    "payoff_0 = np.array([[0, 3], \n",
    "                     [1, 2]])\n",
    "payoff_1 = np.array([[3, 2], \n",
    "                     [0, 1]])\n",
    "\n",
    "pi_alpha = 0. # init policy for player 1\n",
    "pi_beta = 0.9 # init policy for player 2\n",
    "\n",
    "########### TODO:Give nash strategy of given matrix game (2 points) ###########\n",
    "pi_alpha_nash = _  # nash strategy for player 1\n",
    "pi_beta_nash = _ # nash strategy for player 2\n",
    "########### END TODO ###############################################################\n",
    "\n",
    "u_alpha = U(payoff_0)\n",
    "u_beta = U(payoff_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPFoafboX5fD"
   },
   "source": [
    "#### Infinitesimal Gredient Ascent (IGA) (2 points)\n",
    "\n",
    "To find the optimal strategies, here we use the [Infinitesimal Gradient Ascent (IGA)](https://www.sciencedirect.com/science/article/pii/S0004370202001212) to adjust the strategies at each iteration by considering the effect of changing its strategy on its expected payoffs.  These effects can be captured by calculating the partial derivatives of its expected payoff with respect to its strategy.\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \\frac{\\partial V^{1}(\\alpha, \\beta)}{\\partial \\alpha} &=\\beta u^1+\\left(r^1_{12}-r^1_{22}\\right) \\\\ \\frac{\\partial V^{2}(\\alpha, \\beta)}{\\partial \\beta} &=\\alpha u^2+\\left(r^2_{21}-r^2_{22}\\right). \\end{aligned}\n",
    "$$\n",
    "\n",
    "Accodirng the gradient from partial derivatives, players could adjust the strategies in the direction of current gradient with some step size $\\eta$. If $(\\alpha_k, \\beta_k)$ is the strategy pair at $k$th iteration, then using IGA update the strategies would get the new strategies:\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}{\\alpha_{k+1}=\\alpha_{k}+\\eta \\frac{\\partial V^{1}\\left(\\alpha_{k}, \\beta_{k}\\right)}{\\partial \\alpha_{k}}} \\\\ {\\beta_{k+1}=\\beta_{k}+\\eta \\frac{\\partial V^{2}\\left(\\alpha_{k}, \\beta_{k}\\right)}{\\partial \\beta_{k}}}\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jz8iGEn7XjZP"
   },
   "outputs": [],
   "source": [
    "def IGA(pi_alpha,\n",
    "        pi_beta,\n",
    "        payoff_0,\n",
    "        payoff_1,\n",
    "        u_alpha,\n",
    "        u_beta,\n",
    "        iteration=1000, # iteration number\n",
    "        eta=0.01 # step size\n",
    "       ):\n",
    "    pi_alpha_history = [pi_alpha]\n",
    "    pi_beta_history = [pi_beta]\n",
    "    pi_alpha_gradient_history = [0.]\n",
    "    pi_beta_gradient_history = [0.]\n",
    "    for i in range(iteration):\n",
    "        ########### TODO:Implement IGA (2 points) ###########\n",
    "        pi_alpha_gradient = _\n",
    "        pi_beta_gradient = _\n",
    "        pi_alpha_next = _\n",
    "        pi_beta_next = _\n",
    "        ########### END TODO ###############################\n",
    "        pi_alpha = max(0., min(1., pi_alpha_next))\n",
    "        pi_beta = max(0., min(1., pi_beta_next))\n",
    "        pi_alpha_gradient_history.append(pi_alpha_gradient)\n",
    "        pi_beta_gradient_history.append(pi_beta_gradient)\n",
    "        pi_alpha_history.append(pi_alpha)\n",
    "        pi_beta_history.append(pi_beta)\n",
    "    return pi_alpha_history, \\\n",
    "           pi_beta_history, \\\n",
    "           pi_alpha_gradient_history, \\\n",
    "           pi_beta_gradient_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHK9uOF5i1bx"
   },
   "source": [
    "#### WoLF-IGA (2 points)\n",
    "\n",
    "The above IGA algorithm uses constant step size. A specific method for varying the learning rate here is [IGA WoLF (Win or Learn Fast)](https://www.sciencedirect.com/science/article/pii/S0004370202001212),  it allows the step size varies over time. Let $\\alpha^{e}$ and $\\beta^{e}$ represent the equilibrium strategies of two players, now we have new updated rules for WoLF-IGA algorithm:\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}{\\alpha_{k+1}=\\alpha_{k}+\\eta_k^{1} \\frac{\\partial V^{1}\\left(\\alpha_{k}, \\beta_{k}\\right)}{\\partial \\alpha_{k}}} \\\\ {\\beta_{k+1}=\\beta_{k}+\\eta_k^{2}  \\frac{\\partial V^{2}\\left(\\alpha_{k}, \\beta_{k}\\right)}{\\partial \\beta_{k}}}\\end{array}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\eta_{k}^{1}=\\left\\{\\begin{array}{l}{\\eta_{\\min } \\text { if } V^1\\left(\\alpha_{k}, \\beta_{k}\\right)>V^1\\left(\\alpha^{e}, \\beta_{k}\\right)} \\\\ {\\eta_{\\max } \\text { otherwise }}\\end{array}\\right.\n",
    "$$\n",
    "$$\n",
    "\\eta_{k}^{2}=\\left\\{\\begin{array}{l}{\\eta_{\\min } \\text { if } V^2\\left(\\alpha_{k}, \\beta_{k}\\right)>V^2\\left(\\alpha_{k}, \\beta^{e}\\right)} \\\\ {\\eta_{\\max } \\text { otherwise }}\\end{array}\\right.\n",
    "$$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GyeREOn5i69H"
   },
   "outputs": [],
   "source": [
    "def WoLF_IGA(pi_alpha,\n",
    "             pi_beta, \n",
    "             payoff_0, \n",
    "             payoff_1,\n",
    "             u_alpha,\n",
    "             u_beta,\n",
    "             pi_alpha_nash, \n",
    "             pi_beta_nash,\n",
    "             iteration=1000,\n",
    "             eta_min=0.01, # min step size\n",
    "             eta_max=0.04 # max step size \n",
    "            ):\n",
    "    pi_alpha_history = [pi_alpha]\n",
    "    pi_beta_history = [pi_beta]\n",
    "    pi_alpha_gradient_history = [0.]\n",
    "    pi_beta_gradient_history = [0.]\n",
    "    for i in range(iteration):\n",
    "        ########### TODO:Implement WoLF-IGA (2 points) ###########\n",
    "        pi_alpha_gradient = _\n",
    "        pi_beta_gradient = _\n",
    "        pi_alpha_next = _\n",
    "        pi_beta_next = _\n",
    "        ########### END TODO #####################################\n",
    "        pi_alpha = max(0., min(1., pi_alpha_next))\n",
    "        pi_beta = max(0., min(1., pi_beta_next))\n",
    "        pi_alpha_gradient_history.append(pi_alpha_gradient)\n",
    "        pi_beta_gradient_history.append(pi_beta_gradient)\n",
    "        pi_alpha_history.append(pi_alpha)\n",
    "        pi_beta_history.append(pi_beta)\n",
    "    return pi_alpha_history, \\\n",
    "           pi_beta_history, \\\n",
    "           pi_alpha_gradient_history, \\\n",
    "           pi_beta_gradient_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m87DHp5ejEbs"
   },
   "source": [
    "#### IGA-PP (2 points)\n",
    "\n",
    "The IGA agent uses the gradient from other's current strategies to adjust its strategy. Suppose that one player knows the change direction of the other’s strategy,\n",
    "i.e., strategy derivative, in addition to its current strategy.\n",
    "Then the player can forecast the other’s strategy and adjust its strategy in response to the forecasted strategy. Thus the strategy update rules is changed to by using the policy prediction ([IGA-PP](https://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/view/1885)):\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}{\\alpha_{k+1}=\\alpha_{k}+\\eta\\frac{\\partial V^{1}\\left(\\alpha_{k}, \\beta_{k} + \\gamma \\partial_{\\beta}V^{2}\\left(\\alpha_{k}, \\beta_{k}\\right)  \\right)}{\\partial \\alpha_{k}}} \\\\ {\\beta_{k+1}=\\beta_{k}+\\eta  \\frac{\\partial V^{2}\\left(\\alpha_{k} + \\gamma \\partial_{\\alpha} V^{1}\\left(\\alpha_{k}, \\beta_{k} \\right) , \\beta_{k}\\right)}{\\partial \\beta_{k}}}\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdWyTm_FjGqk"
   },
   "outputs": [],
   "source": [
    "def IGA_PP(pi_alpha,\n",
    "           pi_beta,\n",
    "           payoff_0,\n",
    "           payoff_1,\n",
    "           u_alpha,\n",
    "           u_beta,\n",
    "           iteration=10000,\n",
    "           eta=0.01, # step size\n",
    "           gamma=0.01 # step size for policy prediction\n",
    "          ):\n",
    "    pi_alpha_history = [pi_alpha]\n",
    "    pi_beta_history = [pi_beta]\n",
    "    pi_alpha_gradient_history = [0.]\n",
    "    pi_beta_gradient_history = [0.]\n",
    "    for i in range(iteration):\n",
    "        ########### TODO:Implement IGA-PP (2 points) ###########\n",
    "        pi_alpha_gradient = _\n",
    "        pi_beta_gradient = _\n",
    "        pi_alpha_next = _\n",
    "        pi_beta_next = _\n",
    "        ########### END TODO ####################################\n",
    "        pi_alpha = max(0., min(1., pi_alpha_next))\n",
    "        pi_beta = max(0., min(1., pi_beta_next))\n",
    "        pi_alpha_gradient_history.append(pi_alpha_gradient)\n",
    "        pi_beta_gradient_history.append(pi_beta_gradient)\n",
    "        pi_alpha_history.append(pi_alpha)\n",
    "        pi_beta_history.append(pi_beta)\n",
    "    return pi_alpha_history, \\\n",
    "           pi_beta_history, \\\n",
    "           pi_alpha_gradient_history, \\\n",
    "           pi_beta_gradient_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckT05j6hj7KI"
   },
   "source": [
    "#### Run and compare different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9IcCRaxZAowK"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "FONTSIZE = 12\n",
    "\n",
    "# Tool to plot the learning dynamics\n",
    "def plot_dynamics(history_pi_0, history_pi_1, pi_alpha_gradient_history, pi_beta_gradient_history, title=''):\n",
    "    colors = range(len(history_pi_1))\n",
    "    fig = plt.figure(figsize=(6, 5))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    scatter = ax.scatter(history_pi_0, history_pi_1, c=colors, s=1)\n",
    "    ax.scatter(0.5, 0.5, c='r', s=15., marker='*')\n",
    "    colorbar = fig.colorbar(scatter, ax=ax)\n",
    "    colorbar.set_label('Iterations', rotation=270, fontsize=FONTSIZE)\n",
    "\n",
    "    skip = slice(0, len(history_pi_0), 50)\n",
    "    ax.quiver(history_pi_0[skip],\n",
    "              history_pi_1[skip],\n",
    "              pi_alpha_gradient_history[skip],\n",
    "              pi_beta_gradient_history[skip],\n",
    "              units='xy', scale=10., zorder=3, color='blue',\n",
    "              width=0.007, headwidth=3., headlength=4.)\n",
    "\n",
    "    ax.set_ylabel(\"Policy of Player 2\", fontsize=FONTSIZE)\n",
    "    ax.set_xlabel(\"Policy of Player 1\", fontsize=FONTSIZE)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_title(title, fontsize=FONTSIZE+8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPXdMsGFM2b0"
   },
   "source": [
    "We have set up the running code for three algorithms on given matrix game as below. You can run/validate and tune (e.g., try different parameters, observe the convergence and learning dynamics) the results by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fPtKIX4Gj93i"
   },
   "outputs": [],
   "source": [
    "agents = ['IGA', 'WoLF-IGA', 'IGA-PP']\n",
    "\n",
    "for agent in agents:\n",
    "\n",
    "  if agent == 'IGA':\n",
    "      pi_alpha_history, \\\n",
    "      pi_beta_history, \\\n",
    "      pi_alpha_gradient_history, \\\n",
    "      pi_beta_gradient_history = IGA(pi_alpha,\n",
    "                                     pi_beta,\n",
    "                                     payoff_0,\n",
    "                                     payoff_1,\n",
    "                                     u_alpha,\n",
    "                                     u_beta,\n",
    "                                     iteration=1000, # iteration number\n",
    "                                     eta=0.01 # step size\n",
    "                                    )\n",
    "  elif agent == 'WoLF-IGA':\n",
    "      pi_alpha_history, \\\n",
    "      pi_beta_history, \\\n",
    "      pi_alpha_gradient_history, \\\n",
    "      pi_beta_gradient_history = WoLF_IGA(pi_alpha,\n",
    "                                          pi_beta,\n",
    "                                          payoff_0,\n",
    "                                          payoff_1,\n",
    "                                          u_alpha,\n",
    "                                          u_beta,\n",
    "                                          pi_alpha_nash=pi_alpha_nash,\n",
    "                                          pi_beta_nash=pi_beta_nash,\n",
    "                                          iteration=1000, # iteration number\n",
    "                                          eta_min=0.01, # min step size\n",
    "                                          eta_max=0.04 # max step size \n",
    "                                         )\n",
    "\n",
    "\n",
    "  elif agent == 'IGA-PP':\n",
    "      pi_alpha_history, \\\n",
    "      pi_beta_history, \\\n",
    "      pi_alpha_gradient_history, \\\n",
    "      pi_beta_gradient_history = IGA_PP(pi_alpha,\n",
    "                                        pi_beta,\n",
    "                                        payoff_0,\n",
    "                                        payoff_1,\n",
    "                                        u_alpha,\n",
    "                                        u_beta,\n",
    "                                        iteration=10000, # iteration number\n",
    "                                        eta=0.01, # step size\n",
    "                                        gamma=0.01 # step size for policy prediction\n",
    "                                       )\n",
    "\n",
    "\n",
    "  plot_dynamics(pi_alpha_history,\n",
    "                pi_beta_history,\n",
    "                pi_alpha_gradient_history,\n",
    "                pi_beta_gradient_history,\n",
    "                agent)\n",
    "  print('{} Done'.format(agent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BweAzn12aSKm"
   },
   "source": [
    "## Part II: Stochastic Game  (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vEOnliRltth"
   },
   "source": [
    "### Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqrbAu0LeevF"
   },
   "source": [
    "In this part, you are required to implement two agent to play the Stochastic Game, which has non-monotonicity reward and requires exploration to achieve the global optimal. \n",
    "\n",
    "There are $3$ intermediate states before arriving at the final state. The game transition and reward matrices are:\n",
    "\n",
    "![Stochastic Game](https://raw.githubusercontent.com/mlii/mvrl/master/data/sg.png)\n",
    "\n",
    "Given an initial reward matrix (shown in the middle of the above plot), the choice of joint action leads to different branches. For example, the joint action pair (0, 0) will lead to the left branch, while the joint action pair (1, 1) will lead to the branch on the right. Agents can observe the current step number and branch. Zero rewards lead to the termination state (shown as the red cross).\n",
    "\n",
    "The optimal policy is to take the top left action pair (0, 0), and finally take the bottom right action pair (1, 1), resulting in a optimal total payoff of $8$.\n",
    "\n",
    "This game is not easy, because it needs $3$-step exploration to discover the optimal policy, and is hard to deviate from sub-optimal (the right branch). Thus, using a strategic exploration approach is necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjAu3MR3cMTr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class StochasticGame():\n",
    "    def __init__(self, episode_limit=5, good_branches=2, batch_size=None, **kwargs):\n",
    "        # Define the agents\n",
    "        self.n_agents = 2\n",
    "\n",
    "        self.episode_limit = episode_limit\n",
    "\n",
    "        # Define the internal state\n",
    "        self.steps = 0\n",
    "\n",
    "        r_matrix = [[1,1],[1,1]]\n",
    "        self.payoff_values = [r_matrix for _ in range(self.episode_limit)]\n",
    "        self.final_step_diff =[[1,1],[1,4]]\n",
    "\n",
    "        self.branches = 4\n",
    "        self.branch = 0\n",
    "\n",
    "        self.n_actions = len(self.payoff_values[0])\n",
    "\n",
    "        self.good_branches = good_branches\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Returns initial observations and states\"\"\"\n",
    "        self.steps = 0\n",
    "        self.branch = 0\n",
    "        return self.get_obs()\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\" Returns reward, terminated, info \"\"\"\n",
    "        current_branch = 0\n",
    "        if (actions[0], actions[1]) == (0,0):\n",
    "            current_branch = 0\n",
    "        if (actions[0], actions[1]) == (0,1):\n",
    "            current_branch = 1\n",
    "        if (actions[0], actions[1]) == (1,0):\n",
    "            current_branch = 2\n",
    "        if (actions[0], actions[1]) == (1,1):\n",
    "            current_branch = 3\n",
    "\n",
    "        if self.steps == 0:\n",
    "            self.branch = current_branch\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        info[\"good_payoff\"] = 0\n",
    "        info[\"branch\"] = self.branch\n",
    "\n",
    "        if self.good_branches == 4:\n",
    "            reward = 1 if self.branch == current_branch else 0 # Need to follow your branch\n",
    "        elif self.good_branches == 2:\n",
    "            reward = 1 if self.branch in [0,3] and self.branch == current_branch else 0\n",
    "        else:\n",
    "            raise Exception(\"Environment not setup to handle {} good branches\".format(self.good_branches))\n",
    "\n",
    "        if self.episode_limit > 1 and self.steps == self.episode_limit - 1 and self.branch == 0:\n",
    "            info[\"good_payoff\"] = 1\n",
    "            reward = self.final_step_diff[actions[0]][actions[1]]\n",
    "\n",
    "        self.steps += 1\n",
    "\n",
    "        if self.steps < self.episode_limit and reward > 0:\n",
    "            terminated = False\n",
    "        else:\n",
    "            terminated = True\n",
    "\n",
    "        info[\"episode_limit\"] = False\n",
    "\n",
    "        # How often the joint-actions are taken\n",
    "        info[\"action_00\"] = 0\n",
    "        info[\"action_01\"] = 0\n",
    "        info[\"action_10\"] = 0\n",
    "        info[\"action_11\"] = 0\n",
    "        if (actions[0], actions[1]) == (0, 0):\n",
    "            info[\"action_00\"] = 1\n",
    "        if (actions[0], actions[1]) == (0, 1):\n",
    "            info[\"action_01\"] = 1\n",
    "        if (actions[0], actions[1]) == (1, 0):\n",
    "            info[\"action_10\"] = 1\n",
    "        if (actions[0], actions[1]) == (1, 1):\n",
    "            info[\"action_11\"] = 1\n",
    "\n",
    "        return self.get_obs(), [reward] * 2, [terminated] * 2, info\n",
    "\n",
    "    def get_obs(self):\n",
    "        \"\"\" Returns all agent observations in a list \"\"\"\n",
    "        one_hot_step = [0] * (self.episode_limit + 1 + self.branches)\n",
    "        one_hot_step[self.steps] = 1\n",
    "        one_hot_step[self.episode_limit + 1 + self.branch] = 1\n",
    "        return [tuple(one_hot_step) for _ in range(self.n_agents)]\n",
    "\n",
    "    def get_obs_agent(self, agent_id):\n",
    "        \"\"\" Returns observation for agent_id \"\"\"\n",
    "        return self.get_obs()[agent_id]\n",
    "\n",
    "    def get_obs_size(self):\n",
    "        \"\"\" Returns the shape of the observation \"\"\"\n",
    "        return len(self.get_obs_agent(0))\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.get_obs_agent(0)\n",
    "\n",
    "    def get_state_size(self):\n",
    "        \"\"\" Returns the shape of the state\"\"\"\n",
    "        return self.get_obs_size()\n",
    "\n",
    "    def get_total_actions(self):\n",
    "        \"\"\" Returns the total number of actions an agent could ever take \"\"\"\n",
    "        return self.n_actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rp0YBuRqlzfQ"
   },
   "source": [
    "### Example: Random Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "en6btnbxlgW4"
   },
   "source": [
    "A simple agent using random policy is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-Xx81yakYr8"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sample(pi):\n",
    "  return np.random.choice(pi.size, size=1, p=pi)[0]\n",
    "\n",
    "def normalize(pi):\n",
    "    minprob = np.min(pi)\n",
    "    if minprob < 0.0:\n",
    "        pi -= minprob\n",
    "    pi /= np.sum(pi)\n",
    "\n",
    "class BaseQAgent:\n",
    "    def __init__(self, name, action_num=2, phi=0.01, gamma=0.95, episilon=0.1, **kwargs):\n",
    "        self.name = name\n",
    "        self.action_num = action_num\n",
    "        self.episilon = episilon\n",
    "        self.gamma = gamma\n",
    "        self.phi = phi\n",
    "        self.epoch = 0\n",
    "        self.Q = None\n",
    "        self.pi = defaultdict(partial(np.random.dirichlet, [1.0] * self.action_num))\n",
    "\n",
    "    def done(self):\n",
    "        pass\n",
    "\n",
    "    def act(self, observation, exploration=False):\n",
    "        if exploration and random.random() < self.episilon:\n",
    "            return random.randint(0, self.action_num - 1)\n",
    "        else:\n",
    "            return sample(self.pi[observation])\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self, observation, action, reward, next_observation, done):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update_policy(self, observation, action):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDhlnKx8bAEH"
   },
   "source": [
    "### TODO: Implement an agent using Q-Learning (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfTfBBOgbBc4"
   },
   "source": [
    "Q-Learning is a single agent learning algorithm for finding optimal policies in MDPs. The key updating rule is as follwings:\n",
    "\n",
    "$$\n",
    "Q(s, a) \\leftarrow(1-\\phi) Q(s, a)+\\phi\\left(r+\\gamma V\\left(s^{\\prime}\\right)\\right)\n",
    "$$\n",
    "\n",
    "where,\n",
    "$$\n",
    "V(s)=\\max\\left(\\left[Q(s, a)_{a \\in \\mathcal{A}}\\right]\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xz8xTCWRbiXI"
   },
   "outputs": [],
   "source": [
    "class QAgent(BaseQAgent):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__('QAgent', **kwargs)\n",
    "        self.Q = defaultdict(partial(np.random.rand, self.action_num))\n",
    "        self.R = defaultdict(partial(np.zeros, self.action_num))\n",
    "        self.count_R = defaultdict(partial(np.zeros, self.action_num))\n",
    "\n",
    "    def done(self):\n",
    "        self.R.clear()\n",
    "        self.count_R.clear()\n",
    "\n",
    "    def update(self, observation, action, reward, next_observation, done):\n",
    "        self.count_R[observation][action] += 1.0\n",
    "        self.R[observation][action] += (reward - self.R[observation][action]) / self.count_R[observation][action]\n",
    "        Q = self.Q[observation]\n",
    "        V = self.val(next_observation)\n",
    "        \n",
    "        if done:\n",
    "            ########### TODO:Implement Q-Learning (Q updating for termination) (1 point) ###########\n",
    "\n",
    "            ########### END TODO #####################################################\n",
    "        else:\n",
    "            ########### TODO:Implement Q-Learning (Q updating) (1 point) ###########\n",
    "            \n",
    "            ########### END TODO #####################################################\n",
    "        self.update_policy(observation, action)\n",
    "        self.epoch += 1\n",
    "\n",
    "    def val(self, observation):\n",
    "        ########### TODO:Implement Q-Learning (V) (1 point) ###########\n",
    "        \n",
    "        ########### END TODO ##########################################\n",
    "        return v\n",
    "\n",
    "    def update_policy(self, observation, action):\n",
    "        Q = self.Q[observation]\n",
    "        self.pi[observation] = (Q == np.max(Q)).astype(np.double)\n",
    "        self.pi[observation] = self.pi[observation] / np.sum(self.pi[observation])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y34W1E4goX5p"
   },
   "source": [
    "### Test your Q agents on the Stochastic Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FlMO0lF4oZmi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "def rollout(env, agents, exploration=True, max_iter=5000, log_episode_interval=100, verbose=False):\n",
    "  history_reward = []\n",
    "  state_n = env.reset()\n",
    "  episode_reward = 0\n",
    "  episode_count = 0\n",
    "  recorded_episodes = []\n",
    "  recorded_episode_reward = []\n",
    "  for i in range(max_iter):\n",
    "      actions = np.array([agent.act(state, exploration) for state, agent in zip(state_n, agents)])\n",
    "      next_state_n, reward_n, done_n, _ = env.step(actions)\n",
    "      episode_reward += np.mean(reward_n)\n",
    "      for j, (state, reward, next_state, done, agent) in enumerate(zip(state_n, reward_n, next_state_n, done_n, agents)):\n",
    "          agent.update(state, actions[j], reward, next_state, done)\n",
    "      state_n = next_state_n\n",
    "      if np.all(done_n):\n",
    "          state_n = env.reset()\n",
    "          history_reward.append(episode_reward)\n",
    "          episode_reward = 0\n",
    "          episode_count += 1\n",
    "          if (i + 1) %  log_episode_interval == 0:\n",
    "            recorded_episodes.append(i)\n",
    "            episodes_mean_reward = np.mean(history_reward)\n",
    "            recorded_episode_reward.append(episodes_mean_reward)\n",
    "            history_reward = []\n",
    "            if verbose:\n",
    "                print('Iterations {}, Reward {}'.format(i, episodes_mean_reward))\n",
    "  return recorded_episodes, recorded_episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5BiTV-Z_KbY"
   },
   "outputs": [],
   "source": [
    "agent_num = 2\n",
    "action_num = 2\n",
    "\n",
    "runs = 10\n",
    "# store data for each run\n",
    "train_recorded_episodes_log = []\n",
    "train_recorded_episode_reward_log = []\n",
    "test_recorded_episode_reward_log = []\n",
    "\n",
    "for i in range(runs):\n",
    "  ##################################### INITIALISATION ####################################\n",
    "  agents = []\n",
    "  env = StochasticGame()\n",
    "  for i in range(agent_num):\n",
    "      agent = QAgent(action_num=action_num)\n",
    "      agents.append(agent)\n",
    "\n",
    "  ####################################### TRAINING #######################################\n",
    "  train_recorded_episodes, train_recorded_episode_reward = rollout(env=env, \n",
    "                                                                  agents=agents, \n",
    "                                                                  exploration=True, \n",
    "                                                                  max_iter=50000)\n",
    "  # store result for every run\n",
    "  train_recorded_episodes_log.append(train_recorded_episodes)\n",
    "  train_recorded_episode_reward_log.append(train_recorded_episode_reward)\n",
    "\n",
    "  ####################################### TESTING #######################################\n",
    "  test_recorded_episodes, test_recorded_episode_reward = rollout(env=env, \n",
    "                                                               agents=agents, \n",
    "                                                               exploration=False, \n",
    "                                                               max_iter=10, \n",
    "                                                               log_episode_interval=1)\n",
    "  # store result for every run\n",
    "  test_recorded_episode_reward_log.append(np.mean(test_recorded_episode_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLtSWUoL_j_h"
   },
   "outputs": [],
   "source": [
    "####################################### TRAINING #######################################\n",
    "# different episodes returned every time so each learning curve shown separately\n",
    "fig = plt.figure(figsize=(9, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for i in range(runs):\n",
    "  ax.plot(train_recorded_episodes_log[i], train_recorded_episode_reward_log[i], label=f'run {i}')\n",
    "ax.set_title(f\"Train learning Curve for {runs} runs\")\n",
    "ax.set_ylabel(\"Episodic Reward\")\n",
    "ax.set_xlabel(\"Iterations\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "####################################### TESTING #######################################\n",
    "print(f'Test reward is (average over {runs} runs):', np.mean(test_recorded_episode_reward_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2riMNI5E-1H"
   },
   "source": [
    "### TODO: Implement an Advanced Agent to solve the Stochastic Game (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KC8WVPHibrxn"
   },
   "source": [
    "Unless you are extremely lucky, the Q-learning agent implemented above is very hard to succeed in the Stochastic Game. In this part, you are required to implement a really cool agent to play the Stochastic Game. \n",
    "\n",
    "**Hint: You might want to use a strategic exploration approach.**\n",
    "\n",
    "Points will be given based on the performance of your algorithm, e.g., if the test reward of your algorithm is 6, you will be given 6/8*9=6.75 points, since the optimal payoff is 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSRNqF5YoWsS"
   },
   "outputs": [],
   "source": [
    "# You can write any code to implement your CoolAgent, please ouput\n",
    "# action via the act(observation, exploration) method\n",
    "class CoolAgent(BaseQAgent):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__('CoolAgent', **kwargs)\n",
    "\n",
    "    def done(self):\n",
    "        pass\n",
    "\n",
    "    def act(self, observation, exploration):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self, observation, action, reward, next_observation, done):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update_policy(self, observation, action):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNHSt61FFXiT"
   },
   "outputs": [],
   "source": [
    "# Feel Free to write code here to train and tune your cool agents, \n",
    "# and assign the trained agents to cool_agents at the end\n",
    "# ########################################\n",
    "# TODO: Your cool agent training code #############\n",
    "agent_num = 2\n",
    "action_num = 2\n",
    "\n",
    "runs = 10\n",
    "# store data for each run\n",
    "train_cool_recorded_episodes_log = []\n",
    "train_cool_recorded_episode_reward_log = []\n",
    "test_cool_recorded_episode_reward_log = []\n",
    "\n",
    "for i in range(runs):\n",
    "  ##################################### INITIALISATION ####################################\n",
    "  agents = []\n",
    "  env = StochasticGame()\n",
    "  for i in range(agent_num):\n",
    "      agent = CoolAgent(action_num=action_num)\n",
    "      agents.append(agent)\n",
    "\n",
    "  ####################################### TRAINING #######################################\n",
    "  train_cool_recorded_episodes, train_cool_recorded_episode_reward = rollout(env=env, \n",
    "                                                                  agents=agents, \n",
    "                                                                  exploration=True, \n",
    "                                                                  max_iter=70000)\n",
    "  # store result for every run\n",
    "  train_cool_recorded_episodes_log.append(train_cool_recorded_episodes)\n",
    "  train_cool_recorded_episode_reward_log.append(train_cool_recorded_episode_reward)\n",
    "  \n",
    "  ####################################### TESTING #######################################\n",
    "  #########################################\n",
    "  cool_agents = agents\n",
    "  # Cool agent evaluation code, please do not change\n",
    "  cool_env = StochasticGame()\n",
    "  test_cool_recorded_episodes, test_cool_recorded_episode_reward = rollout(env=cool_env, \n",
    "                                                                        agents=cool_agents, \n",
    "                                                                        exploration=False, \n",
    "                                                                        max_iter=10, \n",
    "                                                                        log_episode_interval=1)\n",
    "  # store result for every run\n",
    "  test_cool_recorded_episode_reward_log.append(np.mean(test_cool_recorded_episode_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0o121h6NGIlP"
   },
   "outputs": [],
   "source": [
    "####################################### TRAINING #######################################\n",
    "fig = plt.figure(figsize=(9, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for i in range(runs):\n",
    "  ax.plot(train_cool_recorded_episodes_log[i], train_cool_recorded_episode_reward_log[i], label=f'run {i+1}')\n",
    "ax.set_title(f\"Train learning Curve for {runs} runs\")\n",
    "ax.set_ylabel(\"Episodic Reward\")\n",
    "ax.set_xlabel(\"Iterations\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "####################################### TESTING #######################################\n",
    "print(f'Cool agent\\'s test reward is (average over {runs} runs):', np.mean(test_cool_recorded_episode_reward_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZvD1xn0ICLK"
   },
   "source": [
    "Few words to analysis the results comparing to the Q Agent, and what you have did to improve the performance. (< 300 words)\n",
    "\n",
    "...\n",
    "\n",
    "...\n",
    "\n",
    "...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwvzHzFEp1Xv"
   },
   "source": [
    "## Part III: Cournot Duopoly (12 points)\n",
    "\n",
    "Cournot Duopoly is a classic static game that models the imperfect competition in which multiple firms compete in price and production to capture market share.\n",
    "Since the firms' actions are continuous variables, the game is a continuous action setting.\n",
    "It is a **nonzero-sum game** (neither team-based nor zero-sum) which represents a challenge for current MARL methods.\n",
    "\n",
    "Let $a_i\\in [-A_i,A_i]$ represents the set of actions for agent $i\\in\\{1,2\\ldots, N\\}:=\\mathcal{N}$,\n",
    "where $A_i\\in \\mathbb{R}_{>0}$.\n",
    "Each agent $i$'s reward (profit) is \n",
    "$$\n",
    "R_i(a_i,a_{-i})=g_i(a_i,a_{-i})+ w_i(a_i),\n",
    "$$\n",
    "where\n",
    "$\n",
    "\\partial^{2} g_{i} / \\partial a_{i}^{2}<0, \\partial g_{i} / \\partial a_{-i}<0\n",
    "$,and \n",
    "$\\partial^{2} g_{i} / \\partial a_{i} \\partial a_{-i}<0\n",
    "$.\n",
    "Agents adopt Markov policies as\n",
    "$\n",
    "a_{i} = \\pi_i(a_{-i}).\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDLU7gSZ47Kq"
   },
   "source": [
    "#### TODO: Assume $N=2$, prove that policy $\\pi_i$ is non-increasing. (5 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVy-ZoUs59rK"
   },
   "source": [
    "Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XX9ak9rd6HfZ"
   },
   "source": [
    "#### TODO: Set up Cournot Duopoly game. (2 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0VoU-V147c0"
   },
   "source": [
    "\n",
    "Suppose that \n",
    "$$\n",
    "g_i=a_i(\\alpha -\\beta\\sum_{j\\in\\mathcal{N}}a_j),\n",
    "w_i=\\gamma a_i.\n",
    "$$\n",
    "We choose $A_i=1.0, \\forall i\\in\\mathcal{N}$ and $\\alpha=1.5, \\beta=1.0, \\gamma=-0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_2R3spG77Do"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "class CournotDuopoly(gym.Env):\n",
    "    def __init__(self, agent_num=2, action_range=(-1., 1.)):\n",
    "        self.agent_num = agent_num\n",
    "        self.action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(1,))\n",
    "        self.rewards = np.zeros((self.agent_num,))\n",
    "        self.t = 0\n",
    "\n",
    "        alpha = 1.5\n",
    "        beta = 1.0\n",
    "        gamma = -0.5\n",
    "\n",
    "        def payoff_n_cournot(action_n, i):\n",
    "            \"\"\"\n",
    "            Define the payoff function R_i(a_i,a_{-i}).\n",
    "            :param action_n: (nd.array) a list of all agents' actions, shape is (agent_num,)\n",
    "            :param i: agent index\n",
    "            :return: R_i(a_i,a_{-i})\n",
    "            \"\"\"\n",
    "            ########### TODO: Compute R_i(a_i,a_{-i}) (1 point) ###########\n",
    "            r = _\n",
    "            ########### END TODO ############################\n",
    "            return r\n",
    "\n",
    "        def payoff_n_cournot_derivative(action_n, i):\n",
    "            \"\"\"\n",
    "            Define the partial derivative of the payoff function R_i(a_i,a_{-i}) w.r.t. a_i.\n",
    "            :param action_n: (nd.array) a list of all agents' actions, shape is (agent_num,)\n",
    "            :param i: agent index\n",
    "            :return: \\partial R_i(a_i,a_{-i}) / \\partial a_i\n",
    "            \"\"\"\n",
    "            ########### TODO: Compute \\partial R_i(a_i,a_{-i}) / \\partial a_i (1 point) ###########\n",
    "            dr = _\n",
    "            ########### END TODO ############################\n",
    "            return dr\n",
    "        \n",
    "        self.payoff = payoff_n_cournot\n",
    "        self.payoff_n_derivative = payoff_n_cournot_derivative\n",
    "\n",
    "    def step(self, action_n):\n",
    "        \"\"\"\n",
    "        Define the environment step function.\n",
    "        :param action_n: (nd.array) a list of all agents' actions, shape is (agent_num,)\n",
    "        :return: state_n: (nd.array) a list of all agents' actions, shape is (agent_num,)\n",
    "        :return: reward_n: (nd.array) a list of all agents' states, shape is (agent_num,)\n",
    "        :return: done_n: (nd.array) a list of all agents' done status, shape is (agent_num,)\n",
    "        :return: info: (dict) a dictionary of customized information\n",
    "        \"\"\"\n",
    "        actions = np.array(action_n).reshape((self.agent_num,))\n",
    "        reward_n = np.zeros((self.agent_num,))\n",
    "        payoff_derivative_n = np.zeros((self.agent_num,))\n",
    "        for i in range(self.agent_num):\n",
    "            payoff_derivative_n[i] = self.payoff_n_derivative(actions, i)\n",
    "            reward_n[i] = self.payoff(actions, i)\n",
    "        self.rewards = reward_n\n",
    "        state_n = np.array(list([[0.0 * i] for i in range(self.agent_num)]))\n",
    "        info = {'reward_n': reward_n, 'reward_n_derivative': payoff_derivative_n}\n",
    "        done_n = np.array([True] * self.agent_num)\n",
    "        self.t += 1\n",
    "        # print(\"state_n, reward_n, done_n, info\", state_n, reward_n, done_n, info)\n",
    "        return state_n, reward_n, done_n, info\n",
    "\n",
    "    def reset(self):\n",
    "        return np.array(list([[0.0 * i] for i in range(self.agent_num)]))\n",
    "\n",
    "    def get_rewards(self):\n",
    "        return self.rewards\n",
    "    \n",
    "    def render(self, mode=\"human\", close=False):\n",
    "        pass\n",
    "\n",
    "    def terminate(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcYFl6ZLIxn1"
   },
   "source": [
    "#### TODO: Implement MADDPG agents to play the Cournot Duopoly Game. (3 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrXdbTolJ0QQ"
   },
   "source": [
    "Implement the MADDPG algorithm presented in the paper:\n",
    "[Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/pdf/1706.02275.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9nsdLcEdLFJS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.action_out = nn.Linear(64, 1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        actions = torch.tanh(self.action_out(x))\n",
    "        return actions\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_shape, action_shape):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_shape + action_shape, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.q_out = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state = torch.cat(state, dim=1)\n",
    "        action = torch.cat(action, dim=1)\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        q_value = self.q_out(x)\n",
    "        return q_value\n",
    "\n",
    "\n",
    "class MADDPG:\n",
    "    def __init__(self, n_agents, agent_id, obs_shape=1, action_shape=1):\n",
    "        self.n_agents = n_agents\n",
    "        self.agent_id = agent_id\n",
    "        self.action_shape = action_shape\n",
    "        self.train_step = 0\n",
    "        self.lr_actor = 1e-4\n",
    "        self.lr_critic = 1e-3\n",
    "        self.tau = 0.01\n",
    "        self.gamma = 0.95\n",
    "\n",
    "        # create the network\n",
    "        self.actor_network = Actor()\n",
    "        self.critic_network = Critic(obs_shape * self.n_agents,\n",
    "                                     action_shape * self.n_agents)\n",
    "\n",
    "        # build up the target network\n",
    "        self.actor_target_network = Actor()\n",
    "        self.critic_target_network = Critic(obs_shape * self.n_agents,\n",
    "                                            action_shape * self.n_agents)\n",
    "\n",
    "        # load the weights into the target networks\n",
    "        self.actor_target_network.load_state_dict(self.actor_network.state_dict())\n",
    "        self.critic_target_network.load_state_dict(self.critic_network.state_dict())\n",
    "\n",
    "        # create the optimizer\n",
    "        self.actor_optim = torch.optim.Adam(self.actor_network.parameters(), lr=self.lr_actor)\n",
    "        self.critic_optim = torch.optim.Adam(self.critic_network.parameters(), lr=self.lr_critic)\n",
    "\n",
    "    # soft update\n",
    "    def _soft_update_target_network(self):\n",
    "        for target_param, param in zip(self.actor_target_network.parameters(),\n",
    "                                       self.actor_network.parameters()):\n",
    "            ########### TODO: Soft-update target actor network (0.5 point) ###########\n",
    "            target_param.data.copy_(None)\n",
    "            ########### END TODO ############################\n",
    "\n",
    "        for target_param, param in zip(self.critic_target_network.parameters(),\n",
    "                                       self.critic_network.parameters()):\n",
    "            ########### TODO: Soft-update target critic network (0.5 point) ###########\n",
    "            target_param.data.copy_(None)\n",
    "            ########### END TODO ############################\n",
    "\n",
    "    # update the network\n",
    "    def train(self, transitions, other_agents):\n",
    "        for key in transitions.keys():\n",
    "            transitions[key] = torch.tensor(transitions[key], dtype=torch.float32)\n",
    "        r = transitions['r_%d' % self.agent_id]\n",
    "        o, u, o_next = [], [], []\n",
    "        for agent_id in range(self.n_agents):\n",
    "            o.append(transitions['o_%d' % agent_id])\n",
    "            u.append(transitions['u_%d' % agent_id])\n",
    "            o_next.append(transitions['o_next_%d' % agent_id])\n",
    "\n",
    "        # calculate the target Q value function\n",
    "        u_next = []\n",
    "        with torch.no_grad():\n",
    "            index = 0\n",
    "            for agent_id in range(self.n_agents):\n",
    "                if agent_id == self.agent_id:\n",
    "                    u_next.append(self.actor_target_network(o_next[agent_id]))\n",
    "                else:\n",
    "                    u_next.append(other_agents[index].actor_target_network(o_next[agent_id]))\n",
    "                    index += 1\n",
    "            q_next = self.critic_target_network(o_next, u_next).detach()\n",
    "\n",
    "            ########### TODO: Calculate the target Q value function (0.5 point) ###########\n",
    "            target_q = _\n",
    "            ########### END TODO ############################\n",
    "\n",
    "        # the q loss\n",
    "        q_value = self.critic_network(o, u)\n",
    "        ########### TODO: Calculate the critic loss (0.5 point) ###########\n",
    "        critic_loss = _\n",
    "        ########### END TODO ############################\n",
    "\n",
    "        # the actor loss\n",
    "        u[self.agent_id] = self.actor_network(o[self.agent_id])\n",
    "        ########### TODO: Calculate the actor loss (0.5 point) ###########\n",
    "        actor_loss = _\n",
    "        ########### END TODO ############################\n",
    "        # update the network\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optim.step()\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        self._soft_update_target_network()\n",
    "        self.train_step += 1\n",
    "\n",
    "    def select_action(self, o, noise_rate, epsilon):\n",
    "        if np.random.uniform() < epsilon:\n",
    "            u = np.random.uniform(-1.0, 1.0, self.action_shape)\n",
    "        else:\n",
    "            inputs = torch.tensor(o, dtype=torch.float32).unsqueeze(0)\n",
    "            ########### TODO: Take action based on the actor network (0.5 point) ###########\n",
    "            u = _\n",
    "            ########### END TODO ############################\n",
    "        return u.copy()\n",
    "\n",
    "    def learn(self, transitions, other_agents):\n",
    "        self.train(transitions, other_agents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43hOXeEOPRwT"
   },
   "source": [
    "#### Some useful scripts (please execute).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bXm7HQCsIqQF"
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "class Buffer:\n",
    "    def __init__(self, n_agents=5):\n",
    "        self.n_agents = n_agents\n",
    "        self.size = int(5e5)\n",
    "        # memory management\n",
    "        self.current_size = 0\n",
    "        # create the buffer to store info\n",
    "        self.buffer = dict()\n",
    "        for i in range(self.n_agents):\n",
    "            self.buffer['o_%d' % i] = np.empty([self.size, 1])\n",
    "            self.buffer['u_%d' % i] = np.empty([self.size, 1])\n",
    "            self.buffer['r_%d' % i] = np.empty([self.size])\n",
    "            self.buffer['o_next_%d' % i] = np.empty([self.size, 1])\n",
    "        # thread lock\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    # store the episode\n",
    "    def store_episode(self, o, u, r, o_next):\n",
    "        idxs = self._get_storage_idx(inc=1)\n",
    "        for i in range(self.n_agents):\n",
    "            with self.lock:\n",
    "                self.buffer['o_%d' % i][idxs] = o[i]\n",
    "                self.buffer['u_%d' % i][idxs] = u[i]\n",
    "                self.buffer['r_%d' % i][idxs] = r[i]\n",
    "                self.buffer['o_next_%d' % i][idxs] = o_next[i]\n",
    "    \n",
    "    # sample the data from the replay buffer\n",
    "    def sample(self, batch_size):\n",
    "        temp_buffer = {}\n",
    "        idx = np.random.randint(0, self.current_size, batch_size)\n",
    "        for key in self.buffer.keys():\n",
    "            temp_buffer[key] = self.buffer[key][idx]\n",
    "        return temp_buffer\n",
    "\n",
    "    def _get_storage_idx(self, inc=None):\n",
    "        inc = inc or 1\n",
    "        if self.current_size+inc <= self.size:\n",
    "            idx = np.arange(self.current_size, self.current_size+inc)\n",
    "        elif self.current_size < self.size:\n",
    "            overflow = inc - (self.size - self.current_size)\n",
    "            idx_a = np.arange(self.current_size, self.size)\n",
    "            idx_b = np.random.randint(0, self.current_size, overflow)\n",
    "            idx = np.concatenate([idx_a, idx_b])\n",
    "        else:\n",
    "            idx = np.random.randint(0, self.size, inc)\n",
    "        self.current_size = min(self.size, self.current_size+inc)\n",
    "        if inc == 1:\n",
    "            idx = idx[0]\n",
    "        return idx\n",
    "\n",
    "def evaluate(env, agents, agent_num, evaluate_episodes, evaluate_episode_len):\n",
    "    returns = []\n",
    "    for episode in range(evaluate_episodes):\n",
    "        # reset the environment\n",
    "        s = env.reset()\n",
    "        rewards_n = np.zeros(agent_num)\n",
    "        rs = []\n",
    "        alist = []\n",
    "        rewards1 = 0\n",
    "        for time_step in range(evaluate_episode_len):\n",
    "            actions = []\n",
    "            with torch.no_grad():\n",
    "                for agent_id, agent in enumerate(agents):\n",
    "                    action = agent.select_action(s[agent_id], 0, 0)\n",
    "                    actions.append(action)\n",
    "            s_next, r, done, info = env.step(actions)\n",
    "            if type(info['reward_n']) is list:\n",
    "                rewards_n += np.sum(info['reward_n'])\n",
    "            else:\n",
    "                rewards_n += info['reward_n'].squeeze()\n",
    "\n",
    "            s = s_next\n",
    "        returns.append(rewards_n)\n",
    "    s = env.reset()\n",
    "    mean_return = sum(returns) / evaluate_episodes\n",
    "\n",
    "    return mean_return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Et0OSYA-Ur48"
   },
   "source": [
    "#### Test your implemented MADDPG agent in the Cournot Duopoly Game.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKBz8uINUoL9"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "noise = 0.1\n",
    "epsilon = 0.1\n",
    "episode_limit = 100\n",
    "n_agents = 5\n",
    "batch_size = 256\n",
    "evaluate_rate = 1000\n",
    "time_steps = 20000\n",
    "evaluate_episode_len = 100\n",
    "evaluate_episodes = 100\n",
    "env = CournotDuopoly(agent_num=n_agents)\n",
    "agents = [MADDPG(n_agents, i, obs_shape=1, action_shape=1) for \n",
    "          i in range(n_agents)]\n",
    "buffer = Buffer(n_agents=n_agents)\n",
    "\n",
    "returns = []\n",
    "done = None\n",
    "mean_return_eval = 0.\n",
    "for time_step in tqdm(range(time_steps)):\n",
    "    if time_step % episode_limit == 0 or np.all(done):\n",
    "        s = env.reset()\n",
    "    u = []\n",
    "    actions = []\n",
    "    with torch.no_grad():\n",
    "        for agent_id, agent in enumerate(agents):\n",
    "            action = agent.select_action(s[agent_id], noise, epsilon)\n",
    "            u.append(action)\n",
    "            actions.append(action)\n",
    "\n",
    "    s_next, r, done, info = env.step(actions)\n",
    "    buffer.store_episode(s[:n_agents], u,\n",
    "                         r[:n_agents], s_next[:n_agents])\n",
    "\n",
    "    s = s_next\n",
    "\n",
    "    if buffer.current_size >= batch_size:\n",
    "        transitions = buffer.sample(batch_size)\n",
    "        for agent in agents:\n",
    "            other_agents = agents.copy()\n",
    "            other_agents.remove(agent)\n",
    "            agent.learn(transitions, other_agents)\n",
    "\n",
    "    if time_step == 0 or time_step % evaluate_rate == 0:\n",
    "        mean_return_eval = evaluate(env, agents, n_agents,\n",
    "                                    evaluate_episodes, evaluate_episode_len)\n",
    "        returns.append(mean_return_eval)\n",
    "        print(mean_return_eval)\n",
    "plt.figure()\n",
    "plt.plot(range(len(returns)), np.array(returns).sum(-1))\n",
    "plt.xlabel('episode * ' + str(evaluate_rate / episode_limit))\n",
    "plt.ylabel('average returns')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRzeAImXFfOF"
   },
   "source": [
    "#### TODO: Analyze the performance of your implemented MADDPG algorithm. Describe the reason if it's not playing well.(2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wS5cmNIVdJU_"
   },
   "source": [
    "*Hint: Cournot Monopoly is a nonzero-sum game.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmFiujMlcTZc"
   },
   "source": [
    "Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7quwWZTJlwfV"
   },
   "source": [
    "## Part IV: Deep Multi-Agent Reinforcement Learning (18 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBcOf9ENzRSt"
   },
   "source": [
    "For this part of the coursework, you will implement your choice of any deep reinforcement learning algorithms (e.g., DQN, DRQN, or PPO) in a multi-agent setting and get to see it work on the `Switch-n` environment from ma-gym. Please read [the wiki of ma-gym](https://github.com/koulanurag/ma-gym/wiki/) first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAePzQjrcriF"
   },
   "source": [
    "### Environment Example\n",
    "#### Switch2-v0\n",
    "![Switch-2](https://raw.githubusercontent.com/koulanurag/ma-gym/master/static/gif/Switch2-v0.gif)\n",
    "#### Switch4-v0\n",
    "![Switch-4](https://raw.githubusercontent.com/koulanurag/ma-gym/master/static/gif/Switch4-v0.gif)\n",
    "\n",
    "`Switch-n` is a grid world environment having `n agents` where each agent wants to move their corresponding home location (marked in boxes outlined in same colors).\n",
    "Each agent receives only it's local position coordinates. The challenging part of the game is to pass through the narrow corridor through which only one agent can pass at a time. They need to coordinate to not block the pathway for the other. A reward of +5 is given to each agent for reaching their home cell. The episode ends when both agents has reached their home state or for a maximum of 100 steps in environment.\n",
    "\n",
    "Action Space: `0: Down, 1: Left, 2: Up , 3: Right, 4: Noop`\n",
    "\n",
    "Agent Observation : `Agent Coordinate + Steps in env.`\n",
    "\n",
    "Best Score: `NA`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bRNAhv1QMC1"
   },
   "source": [
    "### Download Requirements and Set the Environment\n",
    "The following command will download the required scripts and set up the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0MTkFoNkLi2"
   },
   "outputs": [],
   "source": [
    "!rm -rf /content/ma-gym  \n",
    "!git clone https://github.com/koulanurag/ma-gym.git \n",
    "%cd /content/ma-gym \n",
    "!pip install -q -e . \n",
    "!apt-get install -y xvfb python-opengl x11-utils > /dev/null 2>&1\n",
    "!pip install pyvirtualdisplay > /dev/null 2>&1\n",
    "!apt-get install x11-utils\n",
    "!apt-get update > /dev/null 2>&1\n",
    "!apt-get install cmake > /dev/null 2>&1\n",
    "!pip install --upgrade setuptools 2>&1\n",
    "!pip install ez_setup > /dev/null 2>&1\n",
    "!pip install -U gym[atari] > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3sYf6AK2d6kK"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import ma_gym\n",
    "from ma_gym.wrappers import Monitor\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment and displaying it\n",
    "To enable video, just do \"env = wrap_env(env)\"\"\n",
    "\"\"\"\n",
    "\n",
    "def show_video():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "  env = Monitor(env, './video', force=True)\n",
    "  return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOzRdQAOu7n_"
   },
   "source": [
    "#### Example of playing Switch2-v0 Using Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QXMuXU52pznF"
   },
   "outputs": [],
   "source": [
    "env = wrap_env(gym.make(\"Switch2-v0\")) # Use \"Switch4-v0\" for the Switch-4 game\n",
    "done_n = [False for _ in range(env.n_agents)]\n",
    "ep_reward = 0\n",
    "\n",
    "obs_n = env.reset()\n",
    "while not all(done_n):\n",
    "    obs_n, reward_n, done_n, info = env.step(env.action_space.sample())\n",
    "    ep_reward += sum(reward_n)\n",
    "    env.render()\n",
    "env.close()\n",
    "# To improve the training efficiency, render() is not necessary during the training.\n",
    "# We provide the render and video code here just want to demonstrate how to debugging and analysis.\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22rC-6i4YztB"
   },
   "source": [
    "### TODO: Implement a Deep MARL Agent to Play Switch2-v0 (12 points)\n",
    "Implement your own choice of any deep MARL algorithms to play the Switch2-v0 game.\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "*   Implement your own algorithms, rather than a simple `import` from any other existing algorithm frameworks.\n",
    "* TensorFlow 2.0+ or PyTorch 1.4+ are recommended.\n",
    "*   Your algorithm has to be Multi-Agent, i.e., policy input should be the observation/ state for each corresponding agent, not for all agents.\n",
    "\n",
    "**Required contents**\n",
    "\n",
    "*   All your codes\n",
    "*   Learning Curve (reward with respect to training iterations/ episodes)\n",
    "*   Description of your code and performance analysis (no more than 500 words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64dJu3WwuBwu"
   },
   "source": [
    "#### Code for Playing Switch2-v0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iin9WBe4uLY2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTZoC30suOe7"
   },
   "source": [
    "#### Plot the Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wgkfaU2RuRaN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBPP9JtCuVEF"
   },
   "source": [
    "#### Analysis on performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AN92fSru0F0"
   },
   "source": [
    "Your reports here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyhtZjqwJtCu"
   },
   "source": [
    "### TODO: Implement a Deep MARL Agent to Play Switch4-v0 (6 points)\n",
    "Implement your own choice of any deep MARL algorithms to play the Switch4-v0 game.\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "Same reuqiremnts as Switch2-v0 game. You can put emphasis on what differences between Switch2-v0 and Switch4-v0 and what improvements you have done.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWZIkzfEuhEE"
   },
   "source": [
    "#### Code for Playing Switch4-v0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vS5uQ0NLun9B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nMHoGAbuoVb"
   },
   "source": [
    "#### Plot the Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9Sr8mHKurcE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yubHxlmQusDs"
   },
   "source": [
    "#### Analysis on Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mOqd9j4uweF"
   },
   "source": [
    "Your reports here."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "43hOXeEOPRwT"
   ],
   "name": "COMP0124_MAAI_2021_Individual_CW_Public.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
